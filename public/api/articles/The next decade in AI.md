# The next decade in AI

## Defintions

*Robust AI* : Intelligence that, while no necessarly superhuman or self-improving (AGI), can be counted on to apply what it knows to a wide range of problems in a **systematic** and **reliable** way, synthezising knowledge fro; a variety of sources such that it can reason **flexibly** and **dynamically** about the world, **transferring** what it learns in one context to another, in a way that we would expect of an ordinary adult.

*Narrow AI*: Systems that perform a single narrow goal extremelly well but often in ways that are extremely centered aruond a single task and not robust and **transferable** to even modestly different circumstances without extensive retraining. 

*Degrees of Learning*: The different tools available to a system to input/infuse/induce knowledge (aka learn)

*Idiosyncrasy*: Systems that lack solid ways of generalizing beyond a space of training examples cannot be trustted in open-ended domains. If you think of each individual systems as a function approximator, currently popular systems tend to be great at memorized examples, and good at many (though not all) examples near the training exeamples - which makes them useful for many applications revolving around classfication. But they are poor when pushed beyond training distribution. 

## Thoughts

- what are the real differences between a **robust** and **AGI**
- is robust AI necessarly a stepway to AGI
- "If we cannot count on our AIto behave reliably, we should no trust it"
  - To what extent is that true?
  - compare this statemtn to the definition of a robust AI

- AI are often compared to human when talking about robustness but there are many limits on this reflection: 
  - in terms of tools at our disposal to explore with have 3d vision, memory, the ability to ask questions to focus our understanding, etc 
    - Make an analogy of the limits of degrees of learning a modern AI has compared to a human.

    - It is like putting a child with no sense of depth in a dark room where the only thing he sees is a screen with images of dogs and not dogs, and then expecting him to be robust.
    - It is like teaching a child by only trying to transmit electric signal directly to the brain as an only way of degrees of learnings

- Pythagorus or the curse of intelligence (being too intelligent to see simple relationships) (Do a little comic a of genius in front of a reflection that creates a whole theory about that it must be aclone from another universe with a power shield in front of it etc etc)
    - Make an AI that learns rules of a system
    - Break down the minimal requirements nodes necessary for breaking a system, via it's logic
        - For example, have an AI that start with one logic node and it tries values to see if it is enough to satisfy the rule
        - If it is not enough, he adds another logic node, and tries to find the rule behind the scheme 
        - repeat until enough logic nodes are added to be able to find a combination that solves the identity


Thoughs on Idiocrary and Excessive dependncy on exact details of training regime
- Models suffer the curse of conspiracy, too smart to see simple relationships or identities

